{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version with windowed-merged datasets making 55x1 input, Dense only model. \n",
    "- data is merged so bins are all mixed up ... \n",
    "- WINDOW_SIZE = 5, gives the merged-sorted result\n",
    "- WINDOW_SIZE = 1, gives the concatenated result, no merging due to WINDOW=Frame size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:55:27.850314Z",
     "start_time": "2025-02-12T13:55:27.833309Z"
    }
   },
   "outputs": [],
   "source": [
    "## All Hyper parameters \n",
    "\n",
    "VISUALISE = False  # plot accumulated datasets or not ? (True/False)\n",
    "DEMO = 'posture'  # example type\n",
    "\n",
    "# Enumeration for each Category, \\/\\/\\/\\/\\/ add new categories here along with data CSV's in folder of same name \n",
    "class_data = {0: 'STANDING', 1: 'SITTING', 2: 'LYING', 3: 'FALLING', 4: 'WALKING'}\n",
    "\n",
    "# Dataset feature formatting \n",
    "WINDOW_SIZE = 8  # window size of frame data: 1,2,3,4,5,6 ... for example.\n",
    "WINDOW_CONCATENATE = False  # concatenate each frame within the window or not (ie merge)\n",
    "WINDOW_SORT = False  # Sort the window data or not (True or False) \n",
    "MIN_POINTS = 5  # Minimum number of point cloud points for a row to be considered\n",
    "EQUALISE_DATA_LENGTHS = False  # Set all categories to have the same rows as the fewest \n",
    "\n",
    "# Data Filtering\n",
    "FILTER = True\n",
    "MAX_HEIGHT = 3\n",
    "MIN_HEIGHT = -4\n",
    "MAX_DISTANCE = 5\n",
    "MIN_DISTANCE = -4\n",
    "LOWEST_POINTS_INCLUDED = False\n",
    "\n",
    "# Training topics \n",
    "NUM_EPOCHS = 1600  # TRAINING: number of epochs to run\n",
    "BATCH_SIZE = 64  # TRAINING: batch size of datasets defining the iterative model update, 64 is OK result. \n",
    "TEST_SIZE_PERCENT = 20  # split dataset % between test(TEST_SIZE_PERCENT) and train(100-TEST_SIZE_PERCENT)\n",
    "LEARNING_RATE = .0001  # Learning rate of the optimizer which determines the step size at each iteration while moving toward a minimum of a loss function\n",
    "F1_SCORE_THRESH = 0.5  # Threshold for converting input into predicted labels for each sample.\n",
    "# Compute binary f1 score, which is defined as the harmonic mean of precision and recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:55:27.913885Z",
     "start_time": "2025-02-12T13:55:27.868317Z"
    },
    "id": "c_exOcWf2tys"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n",
      "torch version: 2.7.1+cu128\n",
      "CUDA version: 12.8\n",
      "GPU: NVIDIA RTX 2000 Ada Generation Laptop GPU, instances:1\n",
      "CPU instances: 20\n",
      "New Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.optim import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.onnx\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}, instances:{torch.cuda.device_count()}\")\n",
    "print(f\"CPU instances: {os.cpu_count()}\")\n",
    "\n",
    "# setup module path for helper functions\n",
    "from modules.helper_functions import accuracy_fn\n",
    "\n",
    "##################################################################\n",
    "# If you have a GPU that supports CUDA, I heavily suggest you    #\n",
    "# set device to CUDA rather than CPU for significantly increased #\n",
    "# model training speeds                                          #\n",
    "##################################################################\n",
    "\n",
    "# device = \"cuda\"\n",
    "device = \"cpu\" \n",
    "torch.device(device)\n",
    "print(f\"New Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for the dataset files and find the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:55:28.598659Z",
     "start_time": "2025-02-12T13:55:28.587146Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup path to data folder\n",
    "data_path = Path(os.getcwd() + \"/dataset\")\n",
    "print(data_path)\n",
    "class_path = Path(f\"{data_path}/classes/\")\n",
    "\n",
    "# Find the class names (according to folder structure)\n",
    "class_names = [f for f in os.listdir(class_path)]\n",
    "\n",
    "file_names = [f for f in list(class_path.rglob(\"*.csv\"))]\n",
    "print(f\"File names: {file_names}\")\n",
    "print(f\"# of Files: {len(file_names)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data from the CSV files and concatenate into single dataframe\n",
    "- Read in the CSV files and concatenate into one large dataset.\n",
    "- Sort the rows by classification column.  \n",
    "- separate the classes, and count how many of each classification are present.  \n",
    "- Visualise the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:56:01.994200Z",
     "start_time": "2025-02-12T13:55:28.883765Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%writefile modules/data_setup.py\n",
    "\"\"\"\n",
    "Read in the data from the CSV files and concatenate into single dataframe\n",
    "\"\"\"\n",
    "\n",
    "## Read in the CSV files and concatenate into one large dataset. \n",
    "## Remove collection information columns and add classification column\n",
    "## Remove any rows with invalid data (not enough points or invalid track ID)\n",
    "df_csv_append = pd.DataFrame()\n",
    "for file in file_names:\n",
    "    df = pd.read_csv(file, encoding='utf-8', engine='python')\n",
    "\n",
    "    # Add tracker columns to datasets that may not have recorded it\n",
    "    required = [\n",
    "        \"posx\", \"posy\", \"posz\",\n",
    "        \"velx\", \"vely\", \"velz\",\n",
    "        \"accx\", \"accy\", \"accz\"\n",
    "    ]\n",
    "\n",
    "    # Add any missing ones with a default of 0\n",
    "    for col in required:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0   # pandas automatically broadcasts the scalar to all rows\n",
    "\n",
    "    classification = os.path.basename(os.path.dirname(file)).upper()\n",
    "    value = -1\n",
    "    for key, val in class_data.items():\n",
    "        if val == classification:\n",
    "            value = key\n",
    "    if 'Session_ID' in df.columns:\n",
    "        df.drop(['Session_ID', 'Recording_Number'], axis=1, inplace=True)\n",
    "    df.insert(0, 'classification', value)\n",
    "    print(f'{file.name}: {df.shape}')\n",
    "    dropped = 0\n",
    "    for index, row in df.iterrows():\n",
    "            # First check if either column exists in the DataFrame\n",
    "        if 'Track ID' in df.columns:\n",
    "            track_id_col = 'Track ID'\n",
    "        elif 'tid' in df.columns:\n",
    "            track_id_col = 'tid'\n",
    "        else:\n",
    "            # Handle the case where neither column exists\n",
    "            print(f\"Warning: Neither 'Track ID' nor 'tid' found in file {file.name}\")\n",
    "            df.drop(index, inplace=True)\n",
    "            dropped += 1\n",
    "            continue\n",
    "            \n",
    "        # Now check the condition using the identified column\n",
    "        if (row.notna().sum() < (9 + 3 * MIN_POINTS)) or not str(float(row[track_id_col])).endswith('.0'):\n",
    "            df.drop(index, inplace=True)\n",
    "            dropped += 1\n",
    "\n",
    "    print(f\"Dropped {dropped} rows for invalid data\")\n",
    "\n",
    "    df_csv_append = pd.concat([df_csv_append, df], ignore_index=True)\n",
    "\n",
    "# sort the rows by classification column. \n",
    "df_all_data = df_csv_append.sort_values(by='classification')\n",
    "\n",
    "# Format of final training dataFrame (0,1,2 : highest, 3,4,5 : Lowest) \n",
    "# Number of high/low points dependant on min number of points available\n",
    "static_columns = ['classification', 'posz', 'velx', 'vely', 'velz', 'accx', 'accy', 'accz']\n",
    "dynamic_columns = [f\"{var}{i}\" for i in range(MIN_POINTS) for var in ['y', 'z', 'snr']]\n",
    "df_sorting_append = pd.DataFrame(columns=static_columns + dynamic_columns)\n",
    "\n",
    "# Extract lowest and highest points from the point cloud data\n",
    "# Form new dataframe using only these points and the target info\n",
    "filtered = 0\n",
    "for index, row in df_all_data.iterrows():\n",
    "    points = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith('pointy') and not np.isnan(row[col]) and not np.isnan(\n",
    "                row[col.replace('pointy', 'pointz')]) and not np.isnan(row[col.replace('pointy', 'snr')]):\n",
    "            points.append([row[col] - row['posy'], row[col.replace('pointy', 'pointz')],\n",
    "                           row[col.replace('pointy', 'snr')]])\n",
    "\n",
    "    df_points = pd.DataFrame(points, columns=['pointy', 'pointz', 'snr'])\n",
    "    df_points = df_points.sort_values(by='pointz')\n",
    "\n",
    "    if FILTER:\n",
    "        df_points = df_points[\n",
    "            (df_points['pointy'] <= MAX_DISTANCE) &\n",
    "            (df_points['pointy'] >= MIN_DISTANCE) &\n",
    "            (df_points['pointz'] <= MAX_HEIGHT) &\n",
    "            (df_points['pointz'] >= MIN_HEIGHT)\n",
    "            ]\n",
    "        if (df_points.shape[0] < MIN_POINTS):\n",
    "            filtered += 1\n",
    "            continue\n",
    "    \n",
    "    if LOWEST_POINTS_INCLUDED:\n",
    "        # Use half points from top and half from bottom\n",
    "        highestPoints = df_points.tail(math.floor(MIN_POINTS / 2))\n",
    "        lowestPoints = df_points.head(math.floor(MIN_POINTS / 2))\n",
    "        \n",
    "        # Create a copy and populate it\n",
    "        df_copy = df_sorting_append.iloc[:0, :].copy()\n",
    "        df_copy.loc[0] = [\n",
    "            row['classification'],\n",
    "            row['posz'],\n",
    "            row['velx'],\n",
    "            row['vely'],\n",
    "            row['velz'],\n",
    "            row['accx'],\n",
    "            row['accy'],\n",
    "            row['accz'],\n",
    "            *highestPoints.values.flatten(),\n",
    "            *lowestPoints.values.flatten()\n",
    "        ]\n",
    "    else:\n",
    "        # Use all MIN_POINTS from top points\n",
    "        allHighPoints = df_points.tail(MIN_POINTS)\n",
    "        \n",
    "        # Create a copy and populate it\n",
    "        df_copy = df_sorting_append.iloc[:0, :].copy()\n",
    "        df_copy.loc[0] = [\n",
    "            row['classification'],\n",
    "            row['posz'],\n",
    "            row['velx'],\n",
    "            row['vely'],\n",
    "            row['velz'],\n",
    "            row['accx'],\n",
    "            row['accy'],\n",
    "            row['accz'],\n",
    "            *allHighPoints.values.flatten()\n",
    "        ]\n",
    "    \n",
    "    df_sorting_append = pd.concat([df_sorting_append, df_copy], ignore_index=True)\n",
    "\n",
    "if (FILTER):\n",
    "    df_sorting_append = df_sorting_append[\n",
    "        (df_sorting_append['posz'] <= MAX_HEIGHT) &\n",
    "        (df_sorting_append['posz'] >= MIN_HEIGHT)]\n",
    "    print(f'Filtered Out {filtered} Items')\n",
    "\n",
    "df_data = df_sorting_append.sort_values(by='classification')\n",
    "\n",
    "_, columns = df_data.shape\n",
    "FEATURE_COUNT = columns - 1\n",
    "\n",
    "print(\"\\nFeature Count: \" + str(FEATURE_COUNT) + \"\\n\")\n",
    "\n",
    "# separate the classes, and count how many of each classification are present. \n",
    "# Remove classification column \n",
    "df_dataList = []  # List of dataFrames, one for each classification\n",
    "\n",
    "for i in range(0, len(class_data)):\n",
    "    df_dataList.append(df_data.query(f'classification == {i}').copy())\n",
    "    df_dataList[i] = df_dataList[i].iloc[:, 1:].reset_index(drop=True)\n",
    "\n",
    "# Set the size of all data frames to be the same as the smallest \n",
    "if EQUALISE_DATA_LENGTHS:\n",
    "    minLength = min(x.shape[0] for x in df_dataList)\n",
    "    for i in range(0, len(class_data)):\n",
    "        df_dataList[i] = df_dataList[i].head(minLength)\n",
    "\n",
    "# output to BIN for CCS importing/testing, \n",
    "# remove the index, export as int32 values.  \n",
    "for i in range(0, len(class_data)):\n",
    "    print(f\"{class_data.get(i)} data shape: {df_dataList[i].shape}\")\n",
    "    # Binary\n",
    "    temp_array = df_dataList[i].to_records(index=False, column_dtypes=\"float32\")\n",
    "    temp_array.tofile(f\"{data_path}/CCS_{class_data.get(i)}_data.bin\")\n",
    "    # CSV\n",
    "    df_dataList[i].to_csv(f\"{data_path}/CCS_{class_data.get(i)}_data.csv\", index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VISUALISE:\n",
    "    currentGraph = 0\n",
    "    graphsPoint = []\n",
    "    graphsVelocity = []\n",
    "    graphsVelocityY = []  # New list for Y velocity graphs\n",
    "\n",
    "    for dfFile in df_dataList:\n",
    "        fig = plt.figure(figsize=(15, 9))  # Made taller to accommodate more graphs\n",
    "        ax2d = fig.add_subplot(2, 3, 1)\n",
    "        axSnr = fig.add_subplot(2, 3, 2, projection='3d')\n",
    "        axVelocityZ = fig.add_subplot(2, 3, 3)\n",
    "        axVelocityY = fig.add_subplot(2, 3, 6)  # New subplot for Y velocity\n",
    "\n",
    "        graphsPoint.append(ax2d)\n",
    "        graphsVelocity.append(axVelocityZ)\n",
    "        graphsVelocityY.append(axVelocityY)  # Add Y velocity graph to list\n",
    "\n",
    "        heighPoints = []\n",
    "        lowPoints = []\n",
    "        velPointsZ = []  # Renamed for clarity\n",
    "        velPointsY = []  # New list for Y velocity points\n",
    "        accPoints = []\n",
    "        snrPoints = []\n",
    "\n",
    "        # Loop through the rows of the DataFrame\n",
    "        for _, row in dfFile.iterrows():\n",
    "            # Extract points for this classification\n",
    "            for i in range(0, (int(MIN_POINTS / 2) - 1)):\n",
    "                y, z, snr = row[f'y{i}'], row[f'z{i}'], row[f'snr{i}']\n",
    "                heighPoints.append((y, z))\n",
    "                snrPoints.append((y, z, snr))\n",
    "            if LOWEST_POINTS_INCLUDED:\n",
    "                for i in range(int(MIN_POINTS / 2), MIN_POINTS - 1):\n",
    "                    y, z, snr = row[f'y{i}'], row[f'z{i}'], row[f'snr{i}']\n",
    "                    lowPoints.append((y, z))\n",
    "                    snrPoints.append((y, z, snr))\n",
    "            else:\n",
    "                # Second half of high points\n",
    "                for i in range(0, (int(MIN_POINTS / 2) - 1)):\n",
    "                    y, z, snr = row[f'y{i}'], row[f'z{i}'], row[f'snr{i}']\n",
    "                    heighPoints.append((y, z))\n",
    "                    snrPoints.append((y, z, snr))\n",
    "\n",
    "            velPointsZ.append((row['posz'], row['velz'], row['accz']))\n",
    "            velPointsY.append((row['vely'], row['accy']))  # Add Y data\n",
    "            accPoints.append((row['posz'], row['accz']))\n",
    "\n",
    "        # Unzip points into x, y, z lists \n",
    "        ysH, zsH = zip(*heighPoints) if heighPoints else ([], [])\n",
    "        y, z, snr = zip(*snrPoints) if snrPoints else ([], [], [])\n",
    "        posz, velz, accz = zip(*velPointsZ) if velPointsZ else ([], [], [])\n",
    "        vely, accy = zip(*velPointsY) if velPointsY else ([], [])  # Unpack Y data\n",
    "\n",
    "        ax2d.scatter(ysH, zsH, c=\"green\", label=\"Heigh Points\", alpha=0.5)\n",
    "        \n",
    "        if LOWEST_POINTS_INCLUDED and len(lowPoints) > 0:\n",
    "            ysL, zsL = zip(*lowPoints) if lowPoints else ([], [])\n",
    "            ax2d.scatter(ysL, zsL, c=\"blue\", label=\"Low Points\", alpha=0.5)\n",
    "\n",
    "        # Z-axis velocity/acceleration plot\n",
    "        axVelocityZ.scatter(posz, velz, c=\"green\", label=\"Velocity Z\", alpha=0.5)\n",
    "        \n",
    "        # Y-axis velocity/acceleration plot (new)\n",
    "        axVelocityY.scatter(vely, accy, c=\"purple\", label=\"Velocity Y\", alpha=0.5)\n",
    "\n",
    "        ax2d.set_ylabel(\"Height (m)\")\n",
    "        ax2d.set_xlabel(\"Distance (m)\")\n",
    "\n",
    "        axSnr.set_ylabel(\"Height (m)\")\n",
    "        axSnr.set_xlabel(\"Distance (m)\")\n",
    "        \n",
    "        axVelocityZ.set_ylabel(\"Velocity Z (m/s)\")\n",
    "        axVelocityZ.set_xlabel(\"Position Z (m)\")\n",
    "        \n",
    "        axVelocityY.set_ylabel(\"Velocity Y (m/s)\")  # Label for new Y plot\n",
    "        axVelocityY.set_xlabel(\"Acceleration Y (m)\")     # Label for new Y plot\n",
    "\n",
    "        cmap = plt.cm.viridis\n",
    "        norm = plt.Normalize(vmin=np.min(snr) if len(snr) > 0 else 0, \n",
    "                            vmax=np.max(snr) if len(snr) > 0 else 1)\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            line_color = cmap(norm(snr[i]))\n",
    "            axSnr.plot([y[i], y[i]], [z[i], z[i]], [0, snr[i]], color=line_color)\n",
    "\n",
    "        ax2d.set_title(f'{class_data.get(currentGraph)} 2D View')\n",
    "        axSnr.set_title(f'{class_data.get(currentGraph)} SNR View')\n",
    "        axVelocityZ.set_title(f'{class_data.get(currentGraph)} Z Velocity')\n",
    "        axVelocityY.set_title(f'{class_data.get(currentGraph)} Y Velocity')  # Title for Y plot\n",
    "\n",
    "        currentGraph = currentGraph + 1\n",
    "\n",
    "    # Adjust all the axes limits to be consistent\n",
    "    minX = min(x.get_xlim()[0] for x in graphsPoint)\n",
    "    maxX = max(x.get_xlim()[1] for x in graphsPoint)\n",
    "    minY = min(x.get_ylim()[0] for x in graphsPoint)\n",
    "    maxY = max(x.get_ylim()[1] for x in graphsPoint)\n",
    "\n",
    "    minXZ = min(x.get_xlim()[0] for x in graphsVelocity)\n",
    "    maxXZ = max(x.get_xlim()[1] for x in graphsVelocity)\n",
    "    minYZ = min(x.get_ylim()[0] for x in graphsVelocity)\n",
    "    maxYZ = max(x.get_ylim()[1] for x in graphsVelocity)\n",
    "    \n",
    "    # New limits for Y velocity plots\n",
    "    minXY = min(x.get_xlim()[0] for x in graphsVelocityY)\n",
    "    maxXY = max(x.get_xlim()[1] for x in graphsVelocityY)\n",
    "    minYY = min(x.get_ylim()[0] for x in graphsVelocityY)\n",
    "    maxYY = max(x.get_ylim()[1] for x in graphsVelocityY)\n",
    "\n",
    "    for x in graphsPoint:\n",
    "        x.set_xlim((minX, maxX))\n",
    "        x.set_ylim((minY, maxY))\n",
    "        x.legend(loc=\"upper left\")\n",
    "\n",
    "    for x in graphsVelocity:\n",
    "        x.set_xlim((minXZ, maxXZ))\n",
    "        x.set_ylim((minYZ, maxYZ))\n",
    "        x.legend(loc=\"upper left\")\n",
    "        \n",
    "    # Set consistent limits for Y velocity graphs\n",
    "    for x in graphsVelocityY:\n",
    "        x.set_xlim((minXY, maxXY))\n",
    "        x.set_ylim((minYY, maxYY))\n",
    "        x.legend(loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature and label arrays (ie X, y)\n",
    "1. Convert dataframe data to lists, and concatenate the frame data into windows. \n",
    "2. Create 2 arrays with X (input), y (output classification)\n",
    "3. Convert lists to np arrays. \n",
    "4. Turn our data into tensors for Pytorch.\n",
    "5. Normalise the feature data so that it is (0 < X < 1)\n",
    "6. Split our data into training and test sets (train a model on the training set to learn the patterns between `X` and `y` and then evaluate those learned patterns on the test dataset).\n",
    "    Use `test_size=0.2` (80% training, 20% testing) and the split happens randomly across the data, so from run to run data will look different. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:56:39.419614Z",
     "start_time": "2025-02-12T13:56:02.008992Z"
    }
   },
   "outputs": [],
   "source": [
    "X_list = []  # features list \n",
    "y_list = []  # labels list \n",
    "\n",
    "fig, axes = plt.subplots(1, len(class_data), figsize=(15, 5))\n",
    "fig.suptitle(f'STANDING versus SITTING versus LYING windowed')\n",
    "\n",
    "for j in range(0, len(class_data)):\n",
    "    LastPoint = len(X_list)  # record where the next data starts\n",
    "\n",
    "    # Convert dataframe data to lists, and associate corresponding label.\n",
    "    for i in range(len(df_dataList[j]) - WINDOW_SIZE):\n",
    "        x = df_dataList[j].loc[i:i + WINDOW_SIZE - 1,\n",
    "            :].unstack().to_frame().T.values.tolist()  # convert windowed datasets to list of lists and merge\n",
    "        if WINDOW_CONCATENATE: x = df_dataList[0].loc[i:i + WINDOW_SIZE - 1,\n",
    "                                   :].values.tolist()  # convert windowed datasets to list of lists and concatenate\n",
    "        x = sum(x, [])  # flatten the window to single list. \n",
    "        if WINDOW_SORT: x.sort()  # sort the window from high to low, hence merging the frames. \n",
    "        X_list.append(x)\n",
    "        y_list.append(j)\n",
    "\n",
    "    # plot windowed data\n",
    "    x_plt = [x for x in range(len(X_list[0]))]\n",
    "    for i in range(LastPoint, len(X_list)):\n",
    "        axes[j].plot(x_plt, (X_list[i]))\n",
    "    axes[j].set(title=f'{class_data.get(j)} dataset: {len(X_list) - LastPoint}x{len(X_list[0])}', xlabel='X',\n",
    "                ylabel='Y')\n",
    "\n",
    "# Convert into numpy arrays. \n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)\n",
    "\n",
    "# Turn data into tensors\n",
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "# Add extra dimension to y so that it matches X.\n",
    "y = torch.unsqueeze(y, 1)\n",
    "print(f\"Shapes for X: {X.shape} and y: {y.shape}\")\n",
    "\n",
    "# View the first example of features and labels\n",
    "print(f\"Values for first sample of X:\\n {X[0]} \\nand corresponding y: {y[0]}\")\n",
    "print(f\"Values for next sample of X:\\n {X[1]} \\nand corresponding y: {y[1]}\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=TEST_SIZE_PERCENT / 100)\n",
    "print(\"New dataset lengths: X_train, X_test, y_train, y_test\", len(X_train), len(X_test), len(y_train), len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the data into DataLoaders for use in training\n",
    "1. convert to dataset format\n",
    "2. convert to dataloaders with batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:56:39.607312Z",
     "start_time": "2025-02-12T13:56:39.592389Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Computing on batches of data, our loss and evaluation metrics will be calculated **per batch** rather than across the whole dataset.\n",
    "# Need to divide our loss and accuracy values by the number of batches in each dataset's respective dataloader. \n",
    "\n",
    "# Create the custom datasets, overload the len and getitem classes\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "test_dataset = MyDataset(X_test, y_test)\n",
    "\n",
    "# Create the dataloaders. \n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=BATCH_SIZE,  # samples per batch \n",
    "                              shuffle=True,  # shuffle data every epoch?\n",
    "                              drop_last=True)  # ignores the last batch (when the number of examples in your dataset is not divisible by your batch_size\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False,  # dont need to shuffle the test data\n",
    "                             drop_last=True)\n",
    "\n",
    "# Dataset dimensions \n",
    "print(f\"Split data into train / test:   Train={100 - TEST_SIZE_PERCENT}% Test={TEST_SIZE_PERCENT}%\")\n",
    "print(f\"X-train, y-train shapes:        {X_train.shape} {y_train.shape}\")\n",
    "print(f\"X-test, y-test shape:           {X_test.shape}  {y_test.shape}\\n\")\n",
    "print(f\"Length of train dataloader:     {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader:      {len(test_dataloader)} batches of {BATCH_SIZE}\")\n",
    "\n",
    "# Check out what's inside the training dataloader\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "print(\"Batch shape (features, labels):\", train_features_batch.shape, train_labels_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeIsGLSn-kdd"
   },
   "source": [
    "### Define the model\n",
    "3 fully connected layers with batch normalization before each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:56:39.765368Z",
     "start_time": "2025-02-12T13:56:39.750362Z"
    },
    "id": "EFDmcdUN7Qzx"
   },
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=input_size)\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=64)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=32)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=16)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=16)\n",
    "        self.fc4 = nn.Linear(in_features=16, out_features=output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.fc4(out)\n",
    "\n",
    "        out = torch.softmax(out, dim=1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the Model\n",
    "`torchinfo` comes with a `summary()` method that takes a PyTorch model as well as an `input_shape` and returns what happens as a tensor moves through the model.\\\n",
    "Additionally we can add all the parameter lengths up to get a more accurate idea of the memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:56:39.939443Z",
     "start_time": "2025-02-12T13:56:39.925439Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model_0 = LinearModel(input_size=WINDOW_SIZE * FEATURE_COUNT, output_size=len(class_data))\n",
    "\n",
    "# more precise memory requirements - lists all parameters and datatypes to retrieve the bits/bytes used. \n",
    "size_model = 0\n",
    "for param in model_0.parameters():\n",
    "    if param.data.is_floating_point():\n",
    "        size_model += param.numel() * torch.finfo(param.data.dtype).bits\n",
    "    else:\n",
    "        size_model += param.numel() * torch.iinfo(param.data.dtype).bits\n",
    "print(f\"Model parameters size: {size_model} bits | {size_model / 8e3:.2f} KB\")\n",
    "\n",
    "# Overall summary of layers with Torchinfo. \n",
    "summary(model_0, input_size=[1, WINDOW_SIZE * FEATURE_COUNT])  # do a test pass through of an example input size \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgvvV2_wG6Yt"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:58:58.230134Z",
     "start_time": "2025-02-12T13:56:40.101039Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aC6ypmuZX8nC",
    "outputId": "a9713ea9-1a81-4531-df50-0b91a2da1332"
   },
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Define model size and send to CPU\n",
    "model_0 = LinearModel(input_size=WINDOW_SIZE * FEATURE_COUNT, output_size=len(class_data))\n",
    "\n",
    "model_0.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# setup the optimizer function  \n",
    "optimizer = SGD(params=model_0.parameters(), lr=LEARNING_RATE)\n",
    "## Alternative optimizer.\n",
    "# optimizer = Adam(params=model_0.parameters(), lr = LEARNING_RATE)   \n",
    "\n",
    "# setup F1 score \n",
    "f1 = MulticlassF1Score(num_classes=len(class_data)).to(device)\n",
    "\n",
    "# Create empty loss lists to track values\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "###############################\n",
    "# Start the main training loop \n",
    "###############################\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train()\n",
    "        # 1. Forward pass\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)  \n",
    "        y = y.squeeze().long()\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss  # accumulatively add up the loss per epoch \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy \n",
    "    test_loss, test_acc = 0, 0\n",
    "    model_0.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    distance = 0\n",
    "\n",
    "    # Calculations on test metrics should happen inside torch.inference_mode()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # 1. Forward pass   \n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred = model_0(X)\n",
    "\n",
    "            # 2. Calculate loss (accumulatively)\n",
    "            y = y.squeeze().long()\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss_fn(test_pred, y)  # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred)\n",
    "\n",
    "            # Convert logits to class labels\n",
    "            predicted_labels = torch.argmax(test_pred, dim=1)\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            all_preds.extend(predicted_labels)\n",
    "            all_labels.extend(y)\n",
    "\n",
    "            # Calculate Hamming Distance Between predictions and correct categories\n",
    "            a = predicted_labels.tolist()\n",
    "            b = y.tolist()\n",
    "\n",
    "            for i in range(len(a)):\n",
    "                if a[i] != b[i]:\n",
    "                    distance += 1\n",
    "\n",
    "                    # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## Print out what's happening in the epoch loop\n",
    "    if epoch % (NUM_EPOCHS / 10) == 0 or epoch == NUM_EPOCHS - 1:\n",
    "        print(f\"EPOCH: {epoch} | F1: {f1(test_pred, y):.5f}\")\n",
    "        print(f\"Train loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n",
    "        print(f'Distance: {distance}')\n",
    "\n",
    "    # keep a history to view loss curves. Detach the tensors from the computation graphs.  \n",
    "    epoch_count.append(epoch)\n",
    "    train_loss_values.append(train_loss.detach())\n",
    "    test_loss_values.append(test_loss.detach())\n",
    "\n",
    "# Need to do convert the tensors to lists otherwise you will get a TypeError when using CUDA\n",
    "if device == \"cuda\":\n",
    "    all_labels = [x.item() for x in all_labels]\n",
    "    all_preds = [x.item() for x in all_preds]\n",
    "    train_loss_values = [x.item() for x in train_loss_values]\n",
    "    test_loss_values = [x.item() for x in test_loss_values]\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DznLhncFCa-9"
   },
   "source": [
    "**export the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:58:58.975539Z",
     "start_time": "2025-02-12T13:58:58.405642Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMCTddxLCeFw",
    "outputId": "1f489630-f09c-44c0-9ad7-52b859878074"
   },
   "outputs": [],
   "source": [
    "import onnx_tool\n",
    "\n",
    "MODEL_PATH = \"my_Linear_model_walk_rel_acc.onnx\"\n",
    "print(MODEL_PATH)\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(model_0, torch.randn(1, WINDOW_SIZE * FEATURE_COUNT).to(device), MODEL_PATH, opset_version=11)\n",
    "onnx_tool.model_profile(MODEL_PATH, save_profile=\"my_Linear-profile_relative.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "2509bf6d86e5424b77b9466e274cbe27d5326092cbf567e955b8a83b8e34c61a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
